The type 1 hypervisor sits on top of the bare metal server and has direct access to the hardware resources. Because of this, the type 1 hypervisor is also known as a bare metal hypervisor. In contrast, the type 2 hypervisor is an application installed on the host operating system.

VMware hypervisors like vSphere, ESXi and ESX.
Microsoft Hyper-V.
Oracle VM Server.
Citrix Hypervisor.

openshift is redhat PaaS (plateform as a server)
openshift is a Redhat container application plateform
openshift origine is based on top of docker containers and kubernetes cluster manager with added developer and operational centric tools that enable repid application developement , deployment and lifecycle management

openshift can intigrate with : 
	- SCM (source code manager) like git,github
	- pipeline
	- registry
	- repository
	- software defined networking 
	- api
	- governance
	
containers are completely isolated env. as they can hve there own process , network interfaces , services just like VM except conatiners share same os kernel.

hardware  <--> OS kernel linux <--> software


OS kernel linux --> ubuntu os ---> docker --> any container with sane linux flaver



hardware -> os -> docker -> container 1 (application - libs,dependncy) ]
							container 2 (application - libs,dependncy)	]

hardware -> os -> hypervisor -> VM [ OS 1 -> (application -  libs,dependncy) ]
								   [ OS 2 ->(application - libs,dependncy)   ]
							 ->  high utilization as multiple Os and kernel running
							 -> more memory size consuption
							 -> high boot time

docker run ansible
docker run nginx
docker run redis
docker run nodejs

docker images :
		-  package, tamplate , plan :  it is similer as VM config file from vagrant or ansible playbooks
		
orchestration technologies :
	-  docker swarm
	-  kubernetes
	-  MESOS by apache
	
	
OCR : openshift container registry

how to install local minishift setup on Virtualbox 
	- download binary from git :
		https://github.com/minishift/minishift/releases
		https://github.com/minishift/minishift
	- then extract and run the command : 
		- minishift config set docker-opt "add-registry=quay.io"
		- 	minishift start
	- we can delete minishift by CMD below : 
		- minishift delete --force --clear-cache
		
18002081800

get the token : 
oc whoami -t
oc get porjects
oc get users
oc adm policy add-cluster-role-to-user cluster-admin <username>

rest call for OKD : 
	- generate auth tokan : 
		oc whoami -t
	-Run below curl command
		curl -k  https://192.168.99.102:8443/oapi/v1/users -H "Authorization: Bearer 8s8Qm56yjKflkpDDXVnsu4xLf6q1jHKTDLcNpCgNvW8"


so in kubernetes we can deploy application as an docker-container where container are created from docker images of the application ,
Images can we pulled from globel docker registry or from openshift container regisrty(OCR) which comes built in OKD

so we can either deploy a pod or multiple pod in deployements and er can expose this pod or deployment with the help of services.


so if we see kubernetes or OKD has two main component :
	- master -  
		it is the actually a manager of kubernetes which has api server, schedules ,ETCD, controler etc.
	- worker - 
		on worker node the actual docker images are hosted and this nodes are managed by single or multiple master nodes 

in OKd we can build  image with :
	- docker file 
	- source-To-Image(S2I)

kube arch:

Master nodes:
[
kube  API server
ETCD
kube scheduler
controller-manager
Node-controller
Replication-controller ]

Worker node:

[
kubelet : runs on each node
kube-proxy : enable communication between the services within the cluster
]

CRI : container runtime interface
OCI : open container initiative

################################## ETCD #######################################

ETCD - Commands (Optional)
(Optional) Additional information about ETCDCTL Utility

ETCDCTL is the CLI tool used to interact with ETCD.

ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3.  By default its set to use Version 2. Each version has different sets of commands.

For example ETCDCTL version 2 supports the following commands:

etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set


Whereas the commands are different in version 3

etcdctl snapshot save 
etcdctl endpoint health
etcdctl get
etcdctl put

To set the right version of API set the environment variable ETCDCTL_API command

export ETCDCTL_API=3



When API version is not set, it is assumed to be set to version 2. And version 3 commands listed above don't work. When API version is set to version 3, version 2 commands listed above don't work.



Apart from that, you must also specify path to certificate files so that ETCDCTL can authenticate to the ETCD API Server. The certificate files are available in the etcd-master at the following path. We discuss more about certificates in the security section of this course. So don't worry if this looks complex:

--cacert /etc/kubernetes/pki/etcd/ca.crt     
--cert /etc/kubernetes/pki/etcd/server.crt     
--key /etc/kubernetes/pki/etcd/server.key


So for the commands I showed in the previous video to work you must specify the ETCDCTL API version and path to certificate files. Below is the final form:



kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 

################################## ETCD ###########################################

kubectl run <podname> --image=<imagename>
kubectl run <podname> --image=<imagename> --dry-run=client -o yaml
kubectl cluster-info
kubectl get node
kubectl get pods

kubectl create/apply -f <yaml file address>
kubectl edit rs <replicaset name> ### to edit replicaset file
kubectl scale rs nginx-rs --replicas=2 ### inline command to change pod number

kubectl set image deployment/<deployment name> <name of the container                                                  >=<newimage name>

kubectl rollout status deployment/<deployment name> ## to check the status of the replicaset

kubectl rollout history deployment/<deployment name> ## to check the histroy of the roleouts

kubectl rollout undo deployment/<deployment name> # to undo the current changes with last changes

minikube service <service name> --url ## to get the service url

-----how to create network namespace in linex

ip netns add red

-- to list the netwrok namespace
ip netns

-- to print network interfaces on host 
ip link

-- how to check link in namespace
ip netns exec red ip link
ip -n red link

-- how to connect namespace
                                                                                                                     


#### taints and toleration ####

kubectl taint nodes <node-name> key=value:taint-effect
taint-effect NoSchedule | PreferNoSchedule | NoExecute

--example
kubectl taint nodes node01 app=blue:NoSchedule

---pod tolertions

apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
spec:
	container:
	 -  name: nginx
		image: nginx
	tolerations:
		- key:"app"
		  operator:"Equal"
		  value:"blue"
		  effect:"NoSchedule"

####### Node Selectores ###########

kubectl label nodes <node-name> <label-key>=<label-value>

kubectl label nodes node01 size=large

apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
spec:
	container:
	 -  name: nginx
		image: nginx
	nodeSelectore:
	  size: Large

##### node affinity ########



#### resource Requirenment and limit ############

apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
spec:
	container:
	 -  name: nginx
		image: nginx
		ports:
		 - containerPort: 8080
	resources:
		requests:
			memory: "1Gi"
			cpu: 1
		limits:
			memory: "2Gi"
			cpu: 2

if you want to set default memory and CPU limits the we can use "LimitRAnge" object which will ensure default allocation of memory and cup to all the pods 

this can only be applyed on namespace

---cpu default limit
apiVersion: v1
kind: LimitRange
metadata:
	name: cpu-resource-constranint
spec:
	limits:
	- default:
	    cpu: 500m
	  defaultRequest:
	    cpu:500m
	  max:
	    cpu: "1"
	  min:
	    cpu: 100m
	  type: Container
	  

---memory default limit
apiVersion: v1
kind: LimitRange
metadata:
	name: memory-resource-constranint
spec:
	limits:
	- default:
	    memory: 1Gi
	  defaultRequest:
	    memory:1Gi
	  max:
	    memory: 1Gi
	  min:
	    memory: 500mi
	  type: Container
	
---to apply resource quota to a namespace ---------

apiVersion: v1
kind: ResourceQuota
metadata:
	name: my-resource-quota
spec:
	hard:
	  request.cpu: 4
	  request.memory: 4Gi
	  limit.cpu: 10
	  limit.memory: 10Gi

---------------- Daemonset----------------------
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: elasticsearch
  namespace: kube-system
  labels:
    k8s-app: elasticsearch
spec:
  selector:
    matchLabels:
      name: elasticsearch-pod
  template:
    metadata: 
      labels:
        name: elasticsearch-pod
    spec:
      containers:
      - name: elasticsearch
        image:  registry.k8s.io/fluentd-elasticsearch:1.20
		
----------------------------------------------------------------


##################### TIPS AND TRICKS START ##################################

Create an NGINX Pod

kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml

Create a deployment

kubectl create deployment --image=nginx nginx

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment YAML file (-o yaml). Don’t create it(–dry-run) and save it to a file.

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml

Make necessary changes to the file (for example, adding more replicas) and then create the deployment.

kubectl create -f nginx-deployment.yaml



OR

In k8s version 1.19+, we can specify the --replicas option to create a deployment with 4 replicas.

kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml

##################### TIPS AND TRICKS END ##################################

################## NAMESPACE / namespace start ######################

kubectl get pods --namespace=<{namespace name} like kube-system>   # to select pods from any other namespace

kubectl create -f <yaml file> --namespace=<name of namespace you want to create pod>

## how to create namespace

apiVersion: v1
kind: Namespace
metadata:
  name:<namespace name>
  
kubectl create -f <yamlfile> 

## how to set other namespace as default

kubectl config set-context $(kubectl config current-context) --namespace=<default namespace name>


kubectl get pods --all-namespaces  ## to view all pods in all namespaces
################## NAMESPACE / namespace end ######################





##### YML file for Pod #####

apiVersion: v1 <for pod and service for ReplicaSet and Deployment it is apps/v1>
kind: pod 
metadata:
  name: myapp-pod
  labels:
    app: myapp
	type: front-end
spec:
  containers:
    - name: nginx-containers
	  image: nginx
	  
	  

##### YML file for Replica controller #####

apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx
          image: nginx
  replicas: 3



##### YML file for Replica Set #####

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-rs
          image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end
	  
	  
##### YML file for deployment #####

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end

##### YML file for service node port #####	  
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 80		## target service port of pod
	  port: 80				## 	port of a service
	  nodePort: 30008		## node/machine port
  selector:
    app: myapp				## lables from deployment
	type: front-end			## lables from deployment
	
##### YML file for service clusterIP to access internal service #####	  
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: ClusterIP			## default type
  ports:
    - targetPort: 80		## target service port of pod
	  port: 80				## 	port of a service
	 
  selector:
    app: myapp				## lables from deployment
	type: front-end			## lables from deployment

##### YML file for service node port #####	  
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: LoadBalancer
  ports:
    - targetPort: 80		## target service port of pod
	  port: 80				## 	port of a service
	  nodePort: 30008		## node/machine port
  selector:
    app: myapp				## lables from deployment
	type: front-end			## lables from deployment


###############  docker and containerd ##################

CRI : container runtime interface , introdused by K8 to make it competable with other container tools 
OCI : opne container initiative (a model or rules that how an image and container run time can build)

dockershim : is introduced by k8 to keep docker competable with k8 because docker is not build in CRI as docker is way older then k8 and very populer container run time 

containerd cli is ctr(with ver limited opetion to work on) and nerdctr.
nerdctr is very similer to docker 

crictl is k8 utility to debug conatainer runtimes




####################################################################

Certification Tips - Imperative Commands with Kubectl
While you would be working mostly the declarative way - using definition files, imperative commands can help in getting one time tasks done quickly, as well as generate a definition template easily. This would help save considerable amount of time during your exams.

Before we begin, familiarize with the two options that can come in handy while working with the below commands:

--dry-run: By default as soon as the command is run, the resource will be created. If you simply want to test your command , use the --dry-run=client option. This will not create the resource, instead, tell you whether the resource can be created and if your command is right.

-o yaml: This will output the resource definition in YAML format on screen.



Use the above two in combination to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.



POD
Create an NGINX Pod

kubectl run nginx --image=nginx



Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml



Deployment
Create a deployment

kubectl create deployment --image=nginx nginx



Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml



Generate Deployment with 4 Replicas

kubectl create deployment nginx --image=nginx --replicas=4



You can also scale a deployment using the kubectl scale command.

kubectl scale deployment nginx --replicas=4

Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml



You can then update the YAML file with the replicas or any other field before creating the deployment.



Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)



Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

(This will not use the pods labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the kubectl expose command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.

Reference:
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands

https://kubernetes.io/docs/reference/kubectl/conventions/

####################################################################


########################### MONITORING ############################

METRICS SERVER - IN MEMORY SERVER

install metric server:
git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git
kubectl create -f .

kubectl top node
kubectl top pods

########################### logging ############################

kubectl logs <pod name> <container name if multiple comtainers are deployed>


######################### Configure Applications ############################

Configuring applications comprises of understanding the following concepts:

Configuring Command and Arguments on applications

Configuring Environment Variables

Configuring Secrets

We will see these next


------ how to pass arg and command to container 

apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
spec:
	container:
	 -  name: nginx
		image: nginx
	    command:	// this will replace ENTERYPOINT IN DOCKER
		- "sleep"
		args:		// This will replace CMD IN DOCKER
		- "5000"

--------creating configmap in applications

imperative way :
		kubectl create configmap \
			<config map name> --from-literal=<key>=<value> \
							  --from-literal=<key>=<value> \
							  --from-literal=<key>=<value>

		kubectl create configmap \
			<config map name> --from-file=<path-to- file>							  
			
		kubectl create configmap \
			<config map name> --from-file=app_config.properties

declarative way : by configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
	name: app-config
data:
	<key>:<value>
	<key>:<value>
	<key>:<value>
	<key>:<value>

kubectl apply -f <<filename>.yaml>
kubectl get configmaps

----inject configmap into pod

apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
spec:
	container:
	 -  name: nginx
		image: nginx
	    command:	
		- "sleep"
		args:		
		- "5000"
		ports:
		- containerPort:8080
		envFrom:
		- configMapRef:
		    name: <configmap name>

---

------------- how to cretae secret -----

imperative way : 
	kubectl create secret generic \
		<secret name> --from-literal=<key>=<value>
					  --from-literal=<key>=<value>
					  --from-literal=<key>=<value>	

	kubectl create secret generic \
		<secret name> --from-file=<filename>				  
					  
declerative way: by secret.yaml file

apiVersion: v1
kind: Secret
metadata:
  name: db-secret
data:
  DB_Host=sql01
  DB_User=root
  DB_Password=password123


kubectl create -f secret.yaml
kubectl get secret

---how to convert plan text to encoded

-to encode value
echo -n '<value to convert>' | <encoding type> // this is to encode value
echo -n 'mysql' | base64

-to decode value
echo -n '<value to decode>' | <encoding type> --decode
echo -n 'bX1zcWw' | base64 --decode

---how to inject secret in pod

apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
spec:
	container:
	 -  name: nginx
		image: nginx
	    command:	
		- "sleep"
		args:		
		- "5000"
		ports:
		- containerPort:8080
		envFrom:
		- secretRef:
		    name: <secret name>
			
--- multiple containers in single pod 

--- initcontainers in pod



######################## Cluster maintenance ########################

kubectl drain node-1   # this will shift all the pods running in node-2 to 	other cluster node 

kubectl cordon node-2  # this cmd will make the node un schedulable so the from that movement no other pods will schedule on that 

kubectl cordon node-2  # this command will undo the above command 


------how to upgrade kubernetes cluster with kubeadm

kubeadm upgrade plan

---UPGRADING MASTER NODE

--first upgrade kubeadm  on master node
apt-get upgrade -y kubeadm=1.12.0-00

-- then we upgrade the other cluster component on master node
kubeadm upgrade apply v1.12.0

-- upgrade kubelet on master node and restart it if it there
apt-get upgrade -y kubelet=1.12.0-00
systemctl restart kubelet

---UPGRADING WORKER NODE

WE WILL UPDATE ONE NODE AT A TIME 
so we will first drain single node and uograde it 

kubectl drain node-1

apt-get upgrade -y kubeadm=1.12.0-00
apt-get upgrade -y kubelet=1.12.0-00
systemctl restart kubelet

kubeadm upgrade node config --kubelet-version v1.12.0 

kubectl uncordon node-1


################## volumns ##########################
----how to mount container path to host path 

apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
spec:
	container:
	 -  name: nginx
		image: nginx
		ports:
		- containerPort:8080
		volumnMounts:
		- mountPath: <container path>				# container path which has to be mount 
		  name: log-volume 							# volume name should be same
		  
	volumns:
	- name: log-volume
	  hostpath:
	    path: <host machine path> 		# host machine path where u will get data locally
		

kubectl replace --force -f /tmp/kubectl-edit-2426351775.yaml

-------how to create PersistentVolume

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path:/pv/log


----storage classes




#########################################################
#########################################################

Kubectx and Kubens – Command line Utilities
Through out the course, you have had to work on several different namespaces in the practice lab environments. In some labs, you also had to switch between several contexts.



While this is excellent for hands-on practice, in a real “live” kubernetes cluster implemented for production, there could be a possibility of often switching between a large number of namespaces and clusters.



This can quickly become and confusing and overwhelming task if you had to rely on kubectl alone.



This is where command line tools such as kubectx and kubens come in to picture.



Reference: https://github.com/ahmetb/kubectx



Kubectx:

With this tool, you don't have to make use of lengthy “kubectl config” commands to switch between contexts. This tool is particularly useful to switch context between clusters in a multi-cluster environment.



Installation:

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx


Syntax:

To list all contexts:

kubectx



To switch to a new context:

kubectx <context_name>



To switch back to previous context:

kubectx -



To see current context:

kubectx -c





Kubens:

This tool allows users to switch between namespaces quickly with a simple command.

Installation:

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens


Syntax:

To switch to a new namespace:

kubens <new_namespace>



To switch back to previous namespace:

kubens -

#########################################################
#########################################################



############################   NETWORKING   ###########################
ip link 
ip addr
ip addr add <ip range from switch> dev eth0 

route

ip route add <destination addr> via <router addr>

ip route add default via <ip where u want all trafic to go >
 
cat /proc/sys/net/ipv4/ip_forword   
----> default value is 0(ip forwording is desabled we can change it to 1 to enable it)


###################   DNS   #######################

cat /etc/resolv.conf       //  to add DNS server details 

cat /etc/nsswitch.conf   //  to change sequance of namespace resolution 

### for ip resolve commands 

ping <ip / host name>

nslookup <ip / host name>    // this only querys the DNs server

dig <ip / host name>

###################################################

###################   network namespaces   #####################

------to create new netwrok namespace on linux server 

ip netns add <name of namespace>

----to view
ip netns

--- to list interfaces 

ip link 

--- to list interfaces inside namespace 

ip netns exec <name of namespace> ip link

ip -n <name of namespace> link

--- arp comman 
arp

 
---############ Connect two namespaces ###################---------------

-- create vertual link

ip link add veth-red type veth peer name veth-blue

ip link set veth-red netns red
ip link set veth-blue netns blue

ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 192.168.15.2 dev veth-blue

ip -n red link set veth-red up
ip -n  blue link set veth-blue up

ip netns exec red ping 192.168.15.2
ip netns exec blue arp
ip -n red arp


####################################################




#############################################################################